{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFmbWaXaLOmY"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Download and prepare dataset\n",
        "vocab_size = 500  # Only consider the top 2k words\n",
        "num_tokens_per_example = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train1, y_train1), (x_val1, y_val1) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "# Concatenate the datasets\n",
        "x_full = np.concatenate((x_train1, x_val1), axis=0)\n",
        "y_full = np.concatenate((y_train1, y_val1), axis=0)\n",
        "\n",
        "# Pad sequences\n",
        "x_full = keras.utils.pad_sequences(x_full, maxlen=num_tokens_per_example)\n",
        "\n",
        "# Split the data into train (60%), temp (40%) for validation and test\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_full, y_full, test_size=0.4, random_state=42)\n",
        "\n",
        "# Split temp into validation (20%) and test (20%) of the original data\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=1/2, random_state=42)\n",
        "\n",
        "# Define hyperparameters\n",
        "embed_dim = 32  # Embedding size for each token.\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feedforward network.\n",
        "num_experts = 2  # Number of experts used in the Switch Transformer.\n",
        "batch_size = 50  # Batch size.\n",
        "learning_rate = 0.01  # Learning rate.\n",
        "dropout_rate = 0.5  # Dropout rate.\n",
        "num_epochs = 20  # Number of epochs.\n",
        "num_tokens_per_batch = batch_size * num_tokens_per_example  # Total number of tokens per batch.\n",
        "\n",
        "# Implement token & position embedding layer\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "# Implement the feedforward network\n",
        "def create_feedforward_network(ff_dim, embed_dim, name=None):\n",
        "    return keras.Sequential(\n",
        "        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)], name=name\n",
        "    )\n",
        "\n",
        "# Implement the load-balanced loss\n",
        "def load_balanced_loss(router_probs, expert_mask):\n",
        "    num_experts = tf.shape(expert_mask)[-1]\n",
        "    density = tf.reduce_mean(expert_mask, axis=0)\n",
        "    density_proxy = tf.reduce_mean(router_probs, axis=0)\n",
        "    loss = tf.reduce_mean(density_proxy * density) * tf.cast((num_experts**2), \"float32\")\n",
        "    return loss\n",
        "\n",
        "# Implement the router as a layer\n",
        "class Router(layers.Layer):\n",
        "    def __init__(self, num_experts, expert_capacity):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_capacity = expert_capacity\n",
        "        self.router_weights = self.add_weight(shape=(embed_dim, num_experts), initializer='random_normal', trainable=True)\n",
        "\n",
        "    def call(self, inputs, attention_weights, training=False):\n",
        "        # Step 1: Compute Router Logits and Probabilities\n",
        "        router_logits = tf.matmul(inputs, self.router_weights)\n",
        "        if training:\n",
        "            router_logits += tf.random.uniform(\n",
        "                shape=router_logits.shape, minval=0.9, maxval=1.1\n",
        "            )\n",
        "        router_probs = tf.nn.softmax(router_logits, axis=-1)\n",
        "\n",
        "        # Step 2: Calculate Token Importance\n",
        "        token_importance = tf.reduce_mean(attention_weights, axis=[1, 2])\n",
        "\n",
        "        # Step 3: Determine the Number of Experts to Route Each Token\n",
        "        num_experts_to_route = tf.cast(token_importance * self.num_experts, tf.int32)\n",
        "\n",
        "        # Step 4: Select Top-k Experts for Each Token\n",
        "        expert_gate, expert_index = tf.nn.top_k(router_probs, k=tf.reduce_max(num_experts_to_route))\n",
        "        expert_index = tf.cast(expert_index, tf.int32)  # Ensure indices are integers\n",
        "        expert_mask = tf.reduce_sum(tf.one_hot(expert_index, self.num_experts), axis=1)\n",
        "\n",
        "        # Step 5: Create Expert Mask\n",
        "        expert_mask = tf.reduce_sum(tf.one_hot(expert_index, self.num_experts), axis=1)\n",
        "\n",
        "        # Step 6: Mask Tokens to Ensure Expert Capacity is Not Exceeded\n",
        "        position_in_expert = tf.cumsum(expert_mask, axis=0) * expert_mask\n",
        "        expert_mask *= tf.cast(tf.less(position_in_expert, self.expert_capacity), tf.float32)\n",
        "\n",
        "        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n",
        "        expert_gate *= expert_mask_flat\n",
        "\n",
        "        combined_tensor = tf.expand_dims(\n",
        "            expert_gate * expert_mask_flat * tf.cast(tf.squeeze(tf.one_hot(expert_index, self.num_experts), 1), tf.float32),\n",
        "            -1,\n",
        "        ) * tf.cast(tf.squeeze(tf.one_hot(position_in_expert, self.expert_capacity), 1), tf.float32)\n",
        "\n",
        "        dispatch_tensor = tf.cast(combined_tensor, \"float32\")\n",
        "        return dispatch_tensor, combined_tensor\n",
        "\n",
        "# Implement a Switch layer\n",
        "class Switch(layers.Layer):\n",
        "    def __init__(self, num_experts, embed_dim, ff_dim, num_tokens_per_batch, capacity_factor=1):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.embed_dim = embed_dim\n",
        "        self.experts = [\n",
        "            create_feedforward_network(ff_dim, embed_dim) for _ in range(num_experts)\n",
        "        ]\n",
        "\n",
        "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
        "        self.router = Router(self.num_experts, self.expert_capacity)\n",
        "\n",
        "    def call(self, inputs, attention_weights):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        num_tokens_per_example = tf.shape(inputs)[1]\n",
        "        num_tokens_per_batch = batch_size * num_tokens_per_example\n",
        "        inputs = tf.reshape(inputs, [num_tokens_per_batch, self.embed_dim])\n",
        "        dispatch_tensor, combine_tensor = self.router(inputs, attention_weights)\n",
        "        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n",
        "        expert_inputs = tf.reshape(\n",
        "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
        "        )\n",
        "        expert_input_list = tf.unstack(expert_inputs, axis=0)\n",
        "        expert_output_list = [\n",
        "            self.experts[idx](expert_input)\n",
        "            for idx, expert_input in enumerate(expert_input_list)\n",
        "        ]\n",
        "        expert_outputs = tf.stack(expert_output_list, axis=1)\n",
        "        expert_outputs_combined = tf.einsum(\n",
        "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
        "        )\n",
        "        outputs = tf.reshape(\n",
        "            expert_outputs_combined,\n",
        "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gMgSeIw3PM0a",
        "outputId": "dcd54432-4006-44e7-9e85-0bc524b0a7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: ml-dtypes, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.4.1 ml-dtypes-0.3.2 tensorboard-2.16.2 tensorflow-2.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "1dfa79169aed472f8a266fd69d1f26cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "# Run experiment with early stopping\n",
        "# Train the classifier with early stopping\n",
        "\n",
        "\n",
        "def classifier():\n",
        "  embed_dim = 32  # Embedding size for each token\n",
        "  num_heads = 2  # Number of attention heads\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "  maxlen = 200\n",
        "  vocab_size=20000\n",
        "  inputs = layers.Input(shape=(200,128))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x = embedding_layer(x_train)\n",
        "  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "  x = transformer_block(x)\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  x = layers.Dropout(0.1)(x)\n",
        "  x = layers.Dense(20, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.1)(x)\n",
        "  outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "  classifier = keras.Model(inputs=x_train, outputs=outputs)\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "  classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "  history = classifier.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "  return classifier, history\n",
        "\n",
        "def run_experiment_with_early_stopping(classifier):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    history = classifier.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "1t2XJ1BLONUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot metrics\n",
        "def plot_metrics(history, axes):\n",
        "    # Plot accuracy\n",
        "    axes[0].plot(history.history['accuracy'], label='Train')\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_title('Model Accuracy')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot loss\n",
        "    axes[1].plot(history.history['loss'], label='Train')\n",
        "    axes[1].plot(history.history['val_loss'], label='Validation')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_title('Model Loss')\n",
        "    axes[1].legend()\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(cm, ax):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], ax=ax)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "    ax.set_title('Confusion Matrix')\n"
      ],
      "metadata": {
        "id": "nZNUfLikR0oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "2w2fxohAWL-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = 200\n",
        "vocab_size=20000\n",
        "inputs = layers.Input(shape=(200,128))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "classifier = keras.Model(inputs=x_train, outputs=outputs)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "classifier.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate),\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"],\n",
        "  )\n",
        "history = classifier.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "\n",
        "# Create the subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot metrics\n",
        "plot_metrics(history, axes)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "sensitivity = recall_score(y_test, y_pred_classes, pos_label=1)\n",
        "specificity = recall_score(y_test, y_pred_classes, pos_label=0)\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Sensitivity: {sensitivity}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(cm, axes[2])\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rpApRSULSXrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "spaHdJCsNsNR",
        "outputId": "60d5f9d2-e31d-4fe2-9348-3cda6feea4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.18.0.dev20240626-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (607.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.5/607.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tf-nightly)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (18.1.1)\n",
            "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tf-nightly)\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.64.1)\n",
            "Collecting tb-nightly~=2.18.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.18.0a20240626-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nightly>=3.2.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-3.4.1.dev2024062703-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nightly>=3.2.0.dev->tf-nightly) (13.7.1)\n",
            "Collecting namex (from keras-nightly>=3.2.0.dev->tf-nightly)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras-nightly>=3.2.0.dev->tf-nightly)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tf-nightly) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.18.0.a->tf-nightly) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.18.0.a->tf-nightly) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.18.0.a->tf-nightly) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.18.0.a->tf-nightly) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.2.0.dev->tf-nightly) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly>=3.2.0.dev->tf-nightly) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.2.0.dev->tf-nightly) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tb-nightly, keras-nightly, tf-nightly\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-nightly-3.4.1.dev2024062703 ml-dtypes-0.4.0 namex-0.0.8 optree-0.11.0 tb-nightly-2.18.0a20240626 tf-nightly-2.18.0.dev20240626\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "4056d617f3294b05813ebd6a925e8def"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}